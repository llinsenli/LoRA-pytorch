{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()  # Suppress all messages except errors\n",
    "import warnings\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of LoRA in DistilBERT\n",
    "\n",
    "This Jupyter notebook demonstrates the application of Low-Rank Adaptation (LoRA) to the DistilBERT model, specifically tailored for sequence classification tasks such as sentiment analysis on the IMDb dataset. LoRA offers an efficient alternative to traditional full model fine-tuning by introducing low-rank matrices that modify only a subset of the model's weights.\n",
    "\n",
    "## Preliminary\n",
    "- Using DistilBertForSequenceClassification\n",
    "\n",
    "We use the `DistilBertForSequenceClassification` model from Hugging Face's Transformers library. Below is the structure of the model:\n",
    "\n",
    "  ```\n",
    "  DistilBertForSequenceClassification(\n",
    "    (distilbert): DistilBertModel(\n",
    "      (embeddings): Embeddings(\n",
    "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "        (position_embeddings): Embedding(512, 768)\n",
    "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        (dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "      (transformer): Transformer(\n",
    "        (layer): ModuleList(\n",
    "          (0-5): 6 x TransformerBlock(\n",
    "            (attention): MultiHeadSelfAttention(\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            )\n",
    "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (ffn): FFN(\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (activation): GELUActivation()\n",
    "            )\n",
    "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
    "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
    "    (dropout): Dropout(p=0.2, inplace=False)\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  The model consists of several parts that contain linear layers, identified for potential adaptation using LoRA:\n",
    "  ```\n",
    "      - Encoder block\n",
    "          - model.distilbert.transformer.layer.attention:\n",
    "              - q_lin, k_lin, v_lin, out_lin\n",
    "          - model.distilbert.transformer.layer.ffn:\n",
    "              - lin1, lin2\n",
    "      - model.pre_classifier\n",
    "      - model.classifier\n",
    "  ```\n",
    "\n",
    "  According to the original LoRA paper, adapting only the attention weights for downstream tasks and freezing the MLP modules (Feed Forward Network) has shown to be effective. We will follow this guidance and apply LoRA to the `q_lin` (query) and `v_lin` (value) linear layers within the attention blocks of each Transformer layer.\n",
    "\n",
    "- Using parametrize.register_parametrization\n",
    "\n",
    "  The `parametrize.register_parametrization` method allows us to redefine how a parameter (like the weights of a linear layer) is computed, without changing the underlying model architecture. This is particularly useful for implementing techniques like LoRA, where we introduce low-rank matrices \\(A\\) and \\(B\\) to modify the original weights \\(W\\) of the model. The re-parameterized weight is calculated as \\(W' = W + BA\\), where \\(B\\) and \\(A\\) are smaller matrices that represent low-rank modifications to \\(W\\).\n",
    "\n",
    "  The following sections of this notebook detail the implementation of the LoRA class, parameterization functions, and application methods to integrate LoRA into the DistilBERT model for efficient fine-tuning on the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adapt LoRA:\n",
      "Requires grad status for model parameters:\n",
      "distilbert.embeddings.word_embeddings.weight: requires_grad=True\n",
      "distilbert.embeddings.position_embeddings.weight: requires_grad=True\n",
      "distilbert.embeddings.LayerNorm.weight: requires_grad=True\n",
      "distilbert.embeddings.LayerNorm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: requires_grad=True\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: requires_grad=True\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: requires_grad=True\n",
      "pre_classifier.weight: requires_grad=True\n",
      "pre_classifier.bias: requires_grad=True\n",
      "classifier.weight: requires_grad=True\n",
      "classifier.bias: requires_grad=True\n",
      "\n",
      "After adapt LoRA:\n",
      "Requires grad status for model parameters:\n",
      "distilbert.embeddings.word_embeddings.weight: requires_grad=False\n",
      "distilbert.embeddings.position_embeddings.weight: requires_grad=False\n",
      "distilbert.embeddings.LayerNorm.weight: requires_grad=False\n",
      "distilbert.embeddings.LayerNorm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.q_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.q_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.q_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.v_lin.parametrizations.weight.original: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.v_lin.parametrizations.weight.0.lora_A: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.v_lin.parametrizations.weight.0.lora_B: requires_grad=True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: requires_grad=False\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: requires_grad=False\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: requires_grad=False\n",
      "pre_classifier.weight: requires_grad=False\n",
      "pre_classifier.bias: requires_grad=False\n",
      "classifier.weight: requires_grad=False\n",
      "classifier.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# LoRA Parametrization Class\n",
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super(LoRAParametrization, self).__init__()\n",
    "        # Initialize A with random Gaussian values and B with zeros as per Section 4.1 of the LoRA paper\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank, features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)  # Normal initialization for A\n",
    "\n",
    "        # Scale factor α/r as described, with no tuning of α beyond this setup\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        delta_w = torch.matmul(self.lora_B, self.lora_A) * self.scale\n",
    "        return original_weights + delta_w\n",
    "\n",
    "# Parameterization Function\n",
    "def linear_layer_parameterization(layer: nn.Linear, device, rank, lora_alpha):\n",
    "    ''' \n",
    "    Update the original weight tensor W in layer with W' = W + B * A * scale using parametrize.register_parametrization()\n",
    "    '''\n",
    "    features_in, features_out = layer.weight.shape # Size of the Linear layer\n",
    "    lora_param = LoRAParametrization(features_in, features_out, rank=rank, alpha=lora_alpha, device=device)\n",
    "    parametrize.register_parametrization(layer, \"weight\", lora_param, unsafe=True)\n",
    "\n",
    "# Function to apply LoRA and freeze parameters and return the model\n",
    "def apply_lora_and_freeze(model, device, config):\n",
    "    '''\n",
    "    Linear layer:\n",
    "    - Encoderblock\n",
    "        - model.distilbert.transformer.layer.attention:\n",
    "            - q_lin, k_lin, v_lin, out_lin\n",
    "        - model.distilbert.transformer.layer.fnn:\n",
    "            - lin1, lin2\n",
    "    - model.pre_classifier\n",
    "    - model.classifier\n",
    "    '''\n",
    "    # Iterate over each transformer layer to apply LoRA based on config\n",
    "    for layer in model.distilbert.transformer.layer:\n",
    "        if config['lora_query']:\n",
    "            linear_layer_parameterization(layer.attention.q_lin, device, config['lora_r'], config['lora_alpha'])\n",
    "        if config['lora_key']:\n",
    "            linear_layer_parameterization(layer.attention.k_lin, device, config['lora_r'], config['lora_alpha'])\n",
    "        if config['lora_value']:\n",
    "            linear_layer_parameterization(layer.attention.v_lin, device, config['lora_r'], config['lora_alpha'])\n",
    "        if config['lora_projection']:\n",
    "            linear_layer_parameterization(layer.attention.out_lin, device, config['lora_r'], config['lora_alpha'])\n",
    "        if config['lora_mlp']:\n",
    "            linear_layer_parameterization(layer.ffn.lin1, device, config['lora_r'], config['lora_alpha'])\n",
    "            linear_layer_parameterization(layer.ffn.lin2, device, config['lora_r'], config['lora_alpha'])\n",
    "\n",
    "    # Apply LoRA to classification heads if enabled\n",
    "    if config['lora_head']:\n",
    "        linear_layer_parameterization(model.pre_classifier, device, config['lora_r'], config['lora_alpha'])\n",
    "        linear_layer_parameterization(model.classifier, device, config['lora_r'], config['lora_alpha'])\n",
    "\n",
    "    # Freeze non-LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "            #print(f'Freezing non-LoRA parameter {name}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Print out the requires_grad_status\n",
    "def print_requires_grad_status(model):\n",
    "    print(\"Requires grad status for model parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, ignore_mismatched_sizes=True )\n",
    "model.to(device)\n",
    "\n",
    "# Configuration for LoRA application for DistilBERT\n",
    "lora_config = {\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_query': True,\n",
    "    'lora_key': False,\n",
    "    'lora_value': True,\n",
    "    'lora_projection': False,\n",
    "    'lora_mlp': False,\n",
    "    'lora_head': False\n",
    "}\n",
    "\n",
    "print(\"Before adapt LoRA:\")\n",
    "print_requires_grad_status(model)\n",
    "# Apply LoRA and freeze parameters\n",
    "model = apply_lora_and_freeze(model, device, lora_config)\n",
    "print(\"\\nAfter adapt LoRA:\")\n",
    "print_requires_grad_status(model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "With the model prepared and LoRA applied, we now proceed to train the model on the IMDb dataset and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_loader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            # Move batch to the appropriate device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)  # Ensure labels are correctly named and used\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)  # This line is just for using the labels for accuracy calculation\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Accuracy: {correct / total:.2f}\")\n",
    "\n",
    "# Load the full IMDb dataset and sample a smaller subset to test the code\n",
    "dataset = load_dataset(\"imdb\")\n",
    "small_train_dataset = dataset['train'].shuffle(seed=42).select(range(200))  # Sample 200 examples for training\n",
    "small_test_dataset = dataset['test'].shuffle(seed=42).select(range(100))   # Sample 100 examples for testing\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "# Define the preprocessing function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512, \n",
    "    )\n",
    "\n",
    "# Tokenize the small datasets\n",
    "small_train_dataset = small_train_dataset.map(preprocess_function, batched=True)\n",
    "small_test_dataset = small_test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch tensors\n",
    "small_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "small_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create DataLoader for both the training and testing datasets\n",
    "train_loader = DataLoader(small_train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(small_test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Optimizer including only parameters with gradients\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Train and Test\n",
    "train(model, train_loader, optimizer, device)\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
